{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "played-mobile",
   "metadata": {},
   "source": [
    "## Linear Regression Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-auction",
   "metadata": {},
   "source": [
    "Given a dataset $$\\{ (x_i, y_i )\\}_{i = 1}^{n}$$ a linear regression model assumes the relationship between the dependent variable $y$ and the vector of regressors $x$ is linear. However in the real world,these relationships are often noisy. So, given a linear regression model with parameters $\\beta$, the relationship we are modeling is $$Y = X\\beta + \\epsilon$$ where \n",
    "$$X = \\begin{bmatrix}\n",
    "x_1 & x_2 & ... & x_n\\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n,d}$$\n",
    "\n",
    "$$Y = \\begin{bmatrix}\n",
    "y_1 & y_2 & ... & y_n\\\\\n",
    "\\end{bmatrix}^{T} \\in \\mathbb{R}^{n}$$\n",
    "\n",
    "$$\\epsilon = \\begin{bmatrix}\n",
    "\\epsilon_1 & \\epsilon_2 & ... & \\epsilon_n\\\\\n",
    "\\end{bmatrix}^{T} \\in \\mathbb{R}^{n}$$\n",
    "\n",
    "Here, $\\epsilon$ corresponds to a noise term, and in the case of linear regression, $\\epsilon \\sim \\mathcal{N}(0, v)$, in other words the error is assumed to be unbiased and normally distributed.\n",
    "\n",
    "Fitting a linear model usually requires to minimizing the error of the model.\n",
    "$$\\begin{align}\n",
    "    Y &= X\\beta + \\epsilon \\\\\n",
    "    Y - X\\beta &= \\epsilon\n",
    "    \\end{align}$$\n",
    "    \n",
    "More formally, linear regression can be formulated as follows:\n",
    "\n",
    "$$\\min_{b}\\|\\epsilon\\|_{2}^{2} = \\min_{b}\\|Y - X\\beta\\|_{2}^{2}$$\n",
    "\n",
    "We can use calculus to solve for $\\beta$ that minimizes the following equation.\n",
    "\n",
    "$$\\begin{align}\n",
    " \\|Y - X\\beta\\|_{2}^{2} &= \\beta^{T}X^{T}X\\beta - 2y^{T}X\\beta + y^{T}y \\\\\n",
    " \\nabla_{\\beta} \\|Y - X\\beta\\|_{2}^{2} &= 2X^{T}X\\beta - 2X^{T}y = 0 \\\\\n",
    " X^{T}X\\beta &= X^{T}y \\\\\n",
    " \\beta &= (X^{T}X)^{-1}X^{T}y\n",
    " \\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-smooth",
   "metadata": {},
   "source": [
    "## Demonstration of single variable linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['animation.html'] = 'html5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create n samples representing the relationship between X and y\n",
    "n = 100\n",
    "x_low, x_high = 0, 5\n",
    "beta_true = 5\n",
    "eps_sigma = 10\n",
    "X = np.linspace(x_low, x_high, num=n)\n",
    "eps = np.random.normal(0, eps_sigma, n)\n",
    "y = X * beta_true + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, y, 'o', label=r'$y = X * \\beta + \\epsilon$: samples')\n",
    "ax.plot(X, X * beta_true, label=r'$y = X * \\beta$ : true relationship', color='red')\n",
    "ax.set(xlabel=r'$X$', ylabel=r'$y = X \\beta + \\epsilon$', title='Linear Regression Demonstration')\n",
    "ax.legend()\n",
    "ax.set_facecolor('pink')\n",
    "plt.savefig('lr_demo.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(X, axis=1)\n",
    "beta_pred = np.linalg.inv(X.T @ X) @ X.T @ y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = y - X * beta_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(e, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(X, y, 'o', label=r'$y = X * \\beta + \\epsilon$: samples', color='gold')\n",
    "ax.plot(X, X * beta_pred, label=r'$\\hat{y} = X * \\hat{\\beta}$ : estimated relationship', color='deeppink')\n",
    "ax.plot(X, X * beta_true, label=r'$y = X * \\beta$ : true relationship', color='seagreen')\n",
    "ax.set(xlabel=r'$X$', ylabel=r'$y = X \\beta + \\epsilon$', title=r'Linear Regression Demonstration')\n",
    "ax.set_facecolor('skyblue')\n",
    "ax.legend()\n",
    "plt.savefig('best_fit.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-comment",
   "metadata": {},
   "source": [
    "### Using Gradient Descent to perform linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-instrumentation",
   "metadata": {},
   "source": [
    "Even though we have an explicit rule to solve for the optimal parameters $\\beta$ given a set of datapoints $\\{ (x_i, y_i )\\}_{i = 1}^{n}$ for linear regression, we can also use the power of gradient descent to solve for an optimal value of $\\beta$ as well. Lets go ahead and demonstrate how we can do the following with Stochastic Gradient Descent as our gradient descent algorithm of choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-orientation",
   "metadata": {},
   "source": [
    "Recall the formulation of Linear Regression:\n",
    "\n",
    "$$\\min_{b}\\|\\epsilon\\|_{2}^{2} = \\min_{b}\\|Y - X\\beta\\|_{2}^{2} = \\min_{b}L(\\beta)$$\n",
    "\n",
    "Now we can use calculus to solve for $\\nabla_{\\beta} L(\\beta) $.\n",
    "\n",
    "$$\\begin{align}\n",
    "L(\\beta) &= \\beta^{T}X^{T}X\\beta - 2y^{T}X\\beta + y^{T}y \\\\\n",
    " \\nabla_{\\beta}L(\\beta) &= 2X^{T}X\\beta - 2X^{T}y\\\\\n",
    " \\end{align}\n",
    "$$\n",
    "\n",
    "With an explicit form for $\\nabla_{\\beta} L(\\beta) $ we can now use SGD to optimize $\\beta$.\n",
    "\n",
    "Recall the update rule for SGD with learning rate $\\lambda$ at timestep $t$\n",
    "\n",
    "$$\\beta_{t} = \\beta_{t - 1} - \\nabla_{\\beta}L(\\beta_{t-1}) * \\lambda$$\n",
    "\n",
    "Now substituting for $\\nabla_{\\beta} L(\\beta) $, we now have an explicit update rule to optimize beta! Here is the update rule below:\n",
    "\n",
    "$$b_{t} = b_{t - 1} - (2X^{T}X\\beta_{t-1} - 2X^{T}y) * \\lambda$$\n",
    "\n",
    "Now, we can optimize $\\beta$ with a random initialization for $\\beta_{0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create n samples representing the relationship between X and y\n",
    "n = 100\n",
    "x_low, x_high = 0, 5\n",
    "beta_true = 2\n",
    "eps_sigma = 1\n",
    "X = np.linspace(x_low, x_high, num=n)\n",
    "eps = np.random.normal(0, eps_sigma, n)\n",
    "y = X * beta_true + eps\n",
    "\n",
    "X = np.expand_dims(X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 100\n",
    "learning_rate = 1e-4\n",
    "beta = np.random.normal(0, 5, 1)\n",
    "hist = []\n",
    "\n",
    "\n",
    "\n",
    "def grad(X, y, b):\n",
    "    return (2 * X.T @ X @ b) - (2 * X.T @ y)\n",
    "\n",
    "def loss(X, y, b):\n",
    "    return np.linalg.norm(y - (X * b), ord=2)\n",
    "\n",
    "for _ in range(num_iters):\n",
    "    beta = beta - grad(X, y, beta) * learning_rate\n",
    "    hist.append((beta, loss(X, y, beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, (pred, loss,) = plt.subplots(2, figsize = (6, 15))\n",
    "fig.suptitle('Demonstration of using SGD to optimize Least Squares model')\n",
    "pred.set(xlim=[0, 5], ylim=[0, 10], xlabel=r'$X$', ylabel=r'$y = X \\beta + \\epsilon$')\n",
    "loss.set(xlim=[0, len(hist)], ylim=[200, 1500], xlabel=r'timesteps (t)', ylabel='\\|\\epsilon\\|_{2}^{2}')\n",
    "\n",
    "def animate(i):\n",
    "    b_i, _ = hist[i]\n",
    "    pred.clear() \n",
    "    pred.set(xlim=[0, 5], ylim=[0, 10], xlabel=r'$X$', ylabel=r'$y = X \\beta + \\epsilon$', title='Linear Regression with SGD $\\lambda$ = 1e-3')\n",
    "    pred.plot(X, y, 'o', label=r'$y = X * \\beta + \\epsilon$: samples', color='gold')\n",
    "    pred.plot(X, X * b_i, label=r'$\\hat{y} = X * \\hat{\\beta}$ : estimated relationship', color='deeppink')\n",
    "    pred.plot(X, X * beta_true, label=r'$y = X * \\beta$ : true relationship', color='red')\n",
    "    pred.set_facecolor('skyblue')\n",
    "    pred.legend()\n",
    "    \n",
    "    loss.clear()\n",
    "    losses = [l for b, l in hist][:i]\n",
    "    loss.set(xlim=[0, len(hist)], ylim=[200, 1500], xlabel=r'timesteps (t)', ylabel=r'loss = $\\|\\epsilon\\|_{2}^{2}$', title=r'loss: $\\|Y - X \\beta \\|_{2}^{2}$ at timestep (t)')\n",
    "    loss.plot(list(range(i)), losses, label='loss at time (t)', color='red')\n",
    "    loss.set_facecolor('skyblue')\n",
    "    loss.legend()\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, num_iters)\n",
    "plt.close()\n",
    "anim.save('linear_regression.gif')\n",
    "HTML(anim.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
